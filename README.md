### Classification-problem-with-Deep-Learning (31st-Dec-2020)
first 2 experiments were related to the use of machine learning techniques, we used most of the algorithms we saw in research papers. There we had a tension of feature engineering as we as the representation of the feature vector was sparse. 
Here we have used Deep Learning which has an edge of selecting the features by itself. That is done by adjusting the weights and increasing the weight of the nodes which has higher probablity and moving the error backward(Error BackPropagation) if we dont get the satisfied answer. 
####  Encoding catagorical features
####   1.   One-Hot Encoding:-   In linear & log-linear models of the form f(x)=xW+b, it is common to think in terms of indicator functions, and assign a unique dimension for each possible feature.  exmple is Bag of words reperesentatation over a vocabulary of 40,000 items, X will be 40,000 dimensional vector, where dimension number 23,372 corresponds to word DOG. 
#### 2.    Here we are taking Distributional semantic approach which is one of the most succesful ideas in Statistical NLP.  in this each feature is represented by a vector & similar words will have some similar vector.
#### If 40,000 features then we would have 40,000 dimension feature vector in case of one hot encoding but in case of WORD EMBEDDING we can represent this as 100 or 200 dimensional vector. The embeddings(the vector representations of each core features) are treated as parameters of the network, and are trained like the other parameters of the function f. Another difference is that we mostly need to extract only CORE FEATURES and not feature combinations. Dimensionality is low.
